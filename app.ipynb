import pandas as pd

# Load the dataset to examine its structure
file_path = '/content/synthetic_event_ticketing.csv'
data = pd.read_csv(file_path)

# Display the first few rows and general information about the dataset
data.head(), data.info()
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Encode categorical features
label_encoders = {}
for column in ['event_type', 'location', 'user_id']:
    label_encoders[column] = LabelEncoder()
    data[column] = label_encoders[column].fit_transform(data[column])

# Separate features and target
X = data[['event_id', 'event_type', 'location', 'user_id']]
y = data['ticket_purchased']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the model
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

accuracy, report

from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler

# Feature Engineering: Create a new feature for event and location popularity
data['event_popularity'] = data.groupby('event_id')['ticket_purchased'].transform('sum')
data['location_popularity'] = data.groupby('location')['ticket_purchased'].transform('sum')

# Update features with new columns
X = data[['event_id', 'event_type', 'location', 'user_id', 'event_popularity', 'location_popularity']]
y = data['ticket_purchased']

# Split the data again with new features
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scaling features (optional, but sometimes beneficial for certain models)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Hyperparameter tuning with GridSearchCV
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10]
}
grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

# Best model after tuning
best_model = grid_search.best_estimator_

# Make predictions with the optimized model
y_pred = best_model.predict(X_test)

# Evaluate the tuned model
tuned_accuracy = accuracy_score(y_test, y_pred)
tuned_report = classification_report(y_test, y_pred)

print("Optimized Model Accuracy:", tuned_accuracy)
print("Optimized Classification Report:\n", tuned_report)

!pip install joblib
import joblib

# Save model
joblib.dump(best_model, 'trained_ticket_model.pkl')

# Save label encoders
joblib.dump(label_encoders, 'label_encoders.pkl')

import joblib
import pandas as pd
import numpy as np

# Load the trained model and label encoders
model = joblib.load('trained_ticket_model.pkl')
label_encoders = joblib.load('label_encoders.pkl')

def predict_ticket_purchase(event_id, event_type, location, user_id):
    # Encode inputs, with fallback for unseen labels
    try:
        event_type_encoded = label_encoders['event_type'].transform([event_type])[0]
    except ValueError:
        event_type_encoded = -1  # Assign a default code for unseen event types

    try:
        location_encoded = label_encoders['location'].transform([location])[0]
    except ValueError:
        location_encoded = -1  # Assign a default code for unseen locations

    try:
        user_id_encoded = label_encoders['user_id'].transform([user_id])[0]
    except ValueError:
        user_id_encoded = -1  # Assign a default code for unseen user IDs

    # Calculate additional features for event and location popularity
    event_popularity = data[data['event_id'] == event_id]['ticket_purchased'].sum()
    location_popularity = data[data['location'] == location]['ticket_purchased'].sum()

    # Create a DataFrame for prediction
    input_data = pd.DataFrame({
        'event_id': [event_id],
        'event_type': [event_type_encoded],
        'location': [location_encoded],
        'user_id': [user_id_encoded],
        'event_popularity': [event_popularity],
        'location_popularity': [location_popularity]
    })

    # Get prediction
    prediction = model.predict(input_data)[0]
    probability = model.predict_proba(input_data)[0][1]  # Probability of purchase

    return prediction, probability



# ... (rest of the code remains the same)

# User inputs from command line
event_id = int(input("Enter Event ID: "))
event_type = input("Enter Event Type (e.g., Theater, Concert): ")
location = input("Enter Location (e.g., San Francisco, Chicago): ")
user_id = input("Enter User ID: ")

# Predict and show results
prediction, probability = predict_ticket_purchase(event_id, event_type, location, user_id)
if prediction == 1:
    print(f"Prediction: Ticket will likely be purchased with a probability of {probability:.2f}")
else:
    print(f"Prediction: Ticket will likely NOT be purchased with a probability of {1 - probability:.2f}")
